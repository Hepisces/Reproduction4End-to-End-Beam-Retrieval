07/29/2024 16:01:47 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='model/deberta-v3-large', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='model/deberta-v3-large', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:01:47 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:07:33 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='model/roberta-large', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='model/roberta-large', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:07:33 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:08:54 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:08:54 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:11:16 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:11:16 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:11:34 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:11:34 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:12:01 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:12:01 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:12:20 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public_datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:12:20 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:14:19 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:14:19 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:14:20 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 16:14:20 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 16:14:24 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 16:18:37 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:18:37 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:18:38 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 16:18:38 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 16:18:41 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 16:40:06 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 16:40:06 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 16:40:08 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 16:40:08 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 16:40:11 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 18:20:08 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 18:20:08 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 18:20:10 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 18:20:10 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 18:20:14 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 18:23:55 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 18:23:55 - INFO - __main__ - device cpu n_gpu 0 distributed training False
07/29/2024 18:23:56 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 18:23:56 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 18:24:00 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 18:26:13 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 18:26:13 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 18:26:14 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 18:26:16 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 18:26:20 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
07/29/2024 18:31:33 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', beam_size=2, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=250, tokenizer_path='E:/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=True, predict_batch_size=1, train_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_train_v1.1.json', predict_file='F:/public datas/NLP/multi_QA/hotpot/hotpot_dev_distractor_v1.json', num_workers=4, do_train=True, do_predict=False, learning_rate=2e-05, warmupsteps=0.1, train_batch_size=8, accumulate_gradients=8, num_train_epochs=20, gradient_checkpointing=True, prefix='retr_hotpot_beam_size2_large', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\retr_hotpot_beam_size2_large-seed42-bsz8-fp16True-lr2e-05-decay0.0-warm0.1-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=0.3, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0.1)
07/29/2024 18:31:33 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 18:31:34 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 18:31:36 - INFO - __main__ - number of trainable parameters: 355363844
07/29/2024 18:31:40 - INFO - __main__ - Start training.... log_steps:113, eval_steps:3391
