07/29/2024 10:47:53 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 10:47:53 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 10:49:07 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 10:49:07 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 10:49:11 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 10:49:12 - INFO - __main__ - number of trainable parameters: 433920007
07/29/2024 14:50:35 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', beam_size=1, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 14:50:35 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 15:07:31 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 15:07:32 - INFO - __main__ - number of trainable parameters: 434016260
07/29/2024 15:26:21 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', beam_size=1, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 15:26:21 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 15:26:25 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 15:26:25 - INFO - __main__ - number of trainable parameters: 434016260
07/29/2024 15:26:35 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 15:26:36 - INFO - __main__ - device cuda n_gpu 1 distributed training False
07/29/2024 15:26:40 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 15:26:40 - INFO - __main__ - number of trainable parameters: 433920007
07/29/2024 16:18:23 - INFO - __main__ - Namespace(no_cuda=False, local_rank=-1, model_name='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', beam_size=1, use_flash_attention=False, flash_attention_type='None', dataset_type='hotpot', mean_passage_len=70, tokenizer_path='MoritzLaurer/deberta-v3-large-zeroshot-v2.0', init_checkpoint='', max_seq_len=512, use_negative_sampling=False, fp16=False, predict_batch_size=1, train_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_train_v1.1.json', predict_file='F:\\public datas\\NLP\\multi_QA\\hotpot\\hotpot_dev_distractor_v1.json', num_workers=4, do_train=False, do_predict=False, learning_rate=5e-06, warmupsteps=0.1, train_batch_size=1, accumulate_gradients=1, num_train_epochs=12, gradient_checkpointing=False, prefix='default_prefix', weight_decay=0.0, temperature=1, output_dir='./output\\07-29-2024\\default_prefix-seed42-bsz1-fp16False-lr5e-06-decay0.0-warm0-valbsz1', adam_epsilon=1e-08, seed=42, eval_period=-1, eval_period_ratio=-1.0, log_period_ratio=0.01, max_grad_norm=2.0, stop_drop=0, use_adam=False, warmup_ratio=0)
07/29/2024 16:18:23 - INFO - __main__ - device cpu n_gpu 1 distributed training False
07/29/2024 16:18:27 - INFO - __main__ - Num of dev batches: 7405
07/29/2024 16:18:27 - INFO - __main__ - number of trainable parameters: 434016260
