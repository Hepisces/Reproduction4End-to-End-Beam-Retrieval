{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r'F:\\GITHUB\\Reproduction4End-to-End-Beam-Retrieval\\original repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--no_cuda] [--local_rank LOCAL_RANK]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--dataset_type DATASET_TYPE]\n",
      "                             [--mean_passage_len MEAN_PASSAGE_LEN]\n",
      "                             [--tokenizer_path TOKENIZER_PATH]\n",
      "                             [--init_checkpoint INIT_CHECKPOINT]\n",
      "                             [--max_seq_len MAX_SEQ_LEN] [--fp16]\n",
      "                             [--predict_batch_size PREDICT_BATCH_SIZE]\n",
      "                             [--train_file TRAIN_FILE]\n",
      "                             [--predict_file PREDICT_FILE]\n",
      "                             [--num_workers NUM_WORKERS] [--do_train]\n",
      "                             [--do_predict] [--learning_rate LEARNING_RATE]\n",
      "                             [--warmupsteps WARMUPSTEPS]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--accumulate_gradients ACCUMULATE_GRADIENTS]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--gradient_checkpointing] [--prefix PREFIX]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--adam_epsilon ADAM_EPSILON] [--seed SEED]\n",
      "                             [--eval_period EVAL_PERIOD]\n",
      "                             [--eval_period_ratio EVAL_PERIOD_RATIO]\n",
      "                             [--log_period_ratio LOG_PERIOD_RATIO]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--stop-drop STOP_DROP] [--use-adam]\n",
      "                             [--warmup-ratio WARMUP_RATIO]\n",
      "ipykernel_launcher.py: error: argument --fp16: ignored explicit argument 'c:\\\\Users\\\\12609\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-155367fIAZ1Wsd239.json'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:1907\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1906\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1907\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_known_args(args, namespace)\n\u001b[0;32m   1908\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:2128\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2127\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[1;32m-> 2128\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m consume_optional(start_index)\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:2050\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[1;34m(start_index)\u001b[0m\n\u001b[0;32m   2049\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignored explicit argument \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 2050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg \u001b[38;5;241m%\u001b[39m explicit_arg)\n\u001b[0;32m   2052\u001b[0m \u001b[38;5;66;03m# if there is no explicit argument, try to match the\u001b[39;00m\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;66;03m# optional's string arguments with the following strings\u001b[39;00m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;66;03m# if successful, exit the loop\u001b[39;00m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --fp16: ignored explicit argument 'c:\\\\Users\\\\12609\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v2-155367fIAZ1Wsd239.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 415\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 415\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m     args \u001b[38;5;241m=\u001b[39m train_args()\n\u001b[0;32m     27\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n",
      "File \u001b[1;32mF:\\GITHUB\\Reproduction4End-to-End-Beam-Retrieval\\original repo\\qa\\config.py:66\u001b[0m, in \u001b[0;36mtrain_args\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--warmup-ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear warmup over warmup_steps.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse_args()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:1874\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1874\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_known_args(args, namespace)\n\u001b[0;32m   1875\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:1909\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1908\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;28mstr\u001b[39m(err))\n\u001b[0;32m   1910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:2640\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2639\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m2\u001b[39m, _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(prog)s\u001b[39;00m\u001b[38;5;124m: error: \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m%\u001b[39m args)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\argparse.py:2627\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2627\u001b[0m _sys\u001b[38;5;241m.\u001b[39mexit(status)\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2146\u001b[0m                                                      value))\n\u001b[0;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1456\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1190\u001b[0m ):\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1193\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1082\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1083\u001b[0m )\n\u001b[0;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from datetime import date\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoConfig, AutoTokenizer,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "from qa.reader_model import Reader\n",
    "from qa.datasets import MHReaderDataset, reader_mhop_collate\n",
    "from utils.utils import load_saved, move_to_cuda, AverageMeter\n",
    "from qa.config import train_args\n",
    "\n",
    "def main():\n",
    "    args = train_args()\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    if args.fp16:\n",
    "        # import apex\n",
    "        # apex.amp.register_half_function(torch, 'einsum')\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        scaler = GradScaler()\n",
    "    date_curr = date.today().strftime(\"%m-%d-%Y\")\n",
    "    model_name = f\"{args.prefix}-seed{args.seed}-bsz{args.train_batch_size}-fp16{args.fp16}-lr{args.learning_rate}-decay{args.weight_decay}-warm{args.warmup_ratio}-valbsz{args.predict_batch_size}\"\n",
    "    args.output_dir = os.path.join(args.output_dir, date_curr, model_name)\n",
    "    tb_path = os.path.join(args.output_dir, \"tblogs\")\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        print(\n",
    "            f\"output directory {args.output_dir} already exists and is not empty.\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "    if not os.path.exists(tb_path):\n",
    "        os.makedirs(tb_path, exist_ok=True)\n",
    "\n",
    "    tb_logger = SummaryWriter(tb_path)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO,\n",
    "                        handlers=[logging.FileHandler(os.path.join(args.output_dir, \"log.txt\")),\n",
    "                                  logging.StreamHandler()])\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(args)\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\",\n",
    "                device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "    if args.accumulate_gradients < 1:\n",
    "        raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
    "            args.accumulate_gradients))\n",
    "\n",
    "    args.train_batch_size = int(\n",
    "        args.train_batch_size / args.accumulate_gradients)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab                          \n",
    "    \n",
    "    bert_config = AutoConfig.from_pretrained(args.model_name)\n",
    "    bert_config.max_position_embeddings = args.max_seq_len\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n",
    "    if args.dataset_type == 'hotpot':\n",
    "        # hotpot format\n",
    "        Sentence_token = \"</e>\"\n",
    "        DOC_token = \"</d>\"\n",
    "        tokenizer.add_tokens([Sentence_token, DOC_token])\n",
    "    model = Reader(bert_config, args.model_name, task_type=args.dataset_type, len_tokenizer=len(tokenizer) if args.dataset_type == 'hotpot' else 0, gradient_checkpointing=args.gradient_checkpointing)\n",
    "    eval_dataset = MHReaderDataset(\n",
    "    tokenizer, args.predict_file, max_len=args.max_seq_len, type=args.dataset_type, is_train=False)\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=args.predict_batch_size, pin_memory=True,\n",
    "    num_workers=args.num_workers, collate_fn=reader_mhop_collate)\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    if args.do_train and args.max_seq_len > bert_config.max_position_embeddings:\n",
    "        raise ValueError(\n",
    "            \"Cannot use sequence length %d because the BERT model \"\n",
    "            \"was only trained up to sequence length %d\" %\n",
    "            (args.max_seq_len, bert_config.max_position_embeddings))\n",
    "\n",
    "    if args.local_rank == -1 or args.local_rank == 0:\n",
    "        logger.info(f\"Num of dev batches: {len(eval_dataloader)}\")\n",
    "\n",
    "    if args.init_checkpoint != \"\":\n",
    "        if args.local_rank == -1 or args.local_rank == 0:\n",
    "            logger.info(f\"begin load trained model from :{args.init_checkpoint}\")\n",
    "        model = load_saved(model, args.init_checkpoint)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if args.local_rank == -1 or args.local_rank == 0:\n",
    "        logger.info(f\"number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    if args.do_train:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = Adam(optimizer_parameters,\n",
    "                         lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "\n",
    "    if args.do_train:\n",
    "        global_step = 0  # gradient update step\n",
    "        batch_step = 0  # forward batch count\n",
    "        best_f1 = 0\n",
    "        train_loss_meter = AverageMeter()\n",
    "        model.train()\n",
    "\n",
    "        train_dataset = MHReaderDataset(tokenizer, args.train_file, max_len=args.max_seq_len, type=args.dataset_type, is_train=True)\n",
    "        if args.local_rank != -1:\n",
    "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, pin_memory=True,\n",
    "                                        num_workers=args.num_workers, sampler=train_sampler, collate_fn=reader_mhop_collate)\n",
    "        else:\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, pin_memory=True,\n",
    "                                        num_workers=args.num_workers, shuffle=True, collate_fn=reader_mhop_collate)\n",
    "    \n",
    "        \n",
    "        t_total = len(train_dataloader) // args.accumulate_gradients * args.num_train_epochs\n",
    "        warmup_steps = t_total * args.warmup_ratio\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        log_steps = int(len(train_dataloader) // args.accumulate_gradients * args.log_period_ratio)\n",
    "        eval_steps = int(len(train_dataloader) // args.accumulate_gradients * args.eval_period_ratio)\n",
    "\n",
    "        if args.local_rank == -1 or args.local_rank == 0:\n",
    "            logger.info(f'Start training.... log_steps:{log_steps}, eval_steps:{eval_steps}')\n",
    "        for epoch in range(int(args.num_train_epochs)):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                batch_step += 1\n",
    "                id = batch.pop('id')\n",
    "                answer = batch.pop('answer')\n",
    "                if 'sentence_num' in batch:\n",
    "                    sentence_num = batch.pop('sentence_num')\n",
    "                batch = move_to_cuda(batch)\n",
    "                if args.fp16:\n",
    "                    with autocast():\n",
    "                        output = model(**batch)\n",
    "                else:\n",
    "                    output = model(**batch)\n",
    "                loss = output['loss'].sum()\n",
    "                if args.accumulate_gradients > 1:\n",
    "                    loss = loss / args.accumulate_gradients\n",
    "                if args.fp16:\n",
    "                    # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    #     scaled_loss.backward()\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                train_loss_meter.update(loss.item())\n",
    "\n",
    "                if (batch_step + 1) % args.accumulate_gradients == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), args.max_grad_norm)\n",
    "                    if args.fp16:\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     amp.master_params(optimizer), args.max_grad_norm)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     model.parameters(), args.max_grad_norm)\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if args.local_rank == -1 or args.local_rank == 0:\n",
    "                        tb_logger.add_scalar('batch_train_loss',\n",
    "                                            loss.item(), global_step)\n",
    "                        tb_logger.add_scalar('smoothed_train_loss',\n",
    "                                            train_loss_meter.avg, global_step)\n",
    "                        tb_logger.add_scalar('ans_span_loss',\n",
    "                                            output['ans_span_loss'].sum().item(), global_step)\n",
    "                        if 'sentence_loss' in output:\n",
    "                            tb_logger.add_scalar('sentence_loss',\n",
    "                                            output['sentence_loss'].sum().item(), global_step)\n",
    "                        if 'ans_type_loss' in output:\n",
    "                            tb_logger.add_scalar('ans_type_loss',\n",
    "                                            output['ans_type_loss'].sum().item(), global_step)\n",
    "\n",
    "                    if global_step % log_steps == 0 and (args.local_rank == -1 or args.local_rank == 0):\n",
    "                        logger.info(\"Step %d Train loss %.8f on epoch=%d, best_metric=%.3f\" % (\n",
    "                        global_step, train_loss_meter.avg, epoch, best_f1))\n",
    "\n",
    "                    if args.eval_period_ratio > 0 and global_step % eval_steps == 0 and (args.local_rank == -1 or args.local_rank == 0):\n",
    "\n",
    "                        metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "                        pred_list = metric['pred_list']\n",
    "                        metric = metric['f1']\n",
    "                        logger.info(\"Step %d Train loss %.8f score %.3f on epoch=%d\" % (\n",
    "                        global_step, train_loss_meter.avg, metric, epoch))\n",
    "\n",
    "                        tb_logger.add_scalar('f1',\n",
    "                                            metric, global_step)\n",
    "                        if best_f1 < metric:\n",
    "                            logger.info(\"Saving model with best score %.3f -> score %.3f on epoch=%d\" %\n",
    "                                        (best_f1, metric, epoch))\n",
    "                            torch.save(model.state_dict(), os.path.join(\n",
    "                                args.output_dir, f\"checkpoint_best.pt\"))\n",
    "                            best_f1 = metric\n",
    "                            json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_best.json\"), 'w'))\n",
    "\n",
    "            if args.local_rank == -1 or args.local_rank == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    args.output_dir, f\"checkpoint_last.pt\"))\n",
    "                metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "                pred_list = metric['pred_list']\n",
    "                metric = metric['f1']\n",
    "                json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_last.json\"), 'w'))\n",
    "                logger.info(\"Step %d Train loss %.8f f1_score %.8f on epoch=%d\" % (\n",
    "                    global_step, train_loss_meter.avg, metric, epoch))\n",
    "\n",
    "                tb_logger.add_scalar('f1',\n",
    "                                            metric, global_step)\n",
    "                \n",
    "                if best_f1 < metric:\n",
    "                    logger.info(\"Saving model with best score %.3f -> score %.3f on epoch=%d\" %\n",
    "                                (best_f1, metric, epoch))\n",
    "                    torch.save(model.state_dict(), os.path.join(\n",
    "                        args.output_dir, f\"checkpoint_best.pt\"))\n",
    "                    best_f1 = metric\n",
    "                    json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_best.json\"), 'w'))\n",
    "\n",
    "        logger.info(\"Training finished!\")\n",
    "\n",
    "    elif args.do_predict:\n",
    "        metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "        json.dump(metric['pred_list'], open(os.path.join(args.output_dir, \"pred.json\"), 'w'))\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks), int(gold_toks == pred_toks), int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def predict(tokenizer, model, eval_dataloader, logger, args):\n",
    "    model.eval()\n",
    "    logger.info(\"begin evaluation\")\n",
    "    pred_list = {}\n",
    "    total_num = 0\n",
    "    metrics = {\"em\": 0,\n",
    "            \"f1\": 0,\n",
    "            \"prec\": 0,\n",
    "            \"recall\": 0\n",
    "            }\n",
    "    if args.dataset_type == 'hotpot':\n",
    "        metrics.update({\n",
    "            \"sp_em\": 0,\n",
    "            \"sp_f1\": 0,\n",
    "            \"sp_prec\": 0,\n",
    "            \"sp_recall\": 0,\n",
    "            \"joint_em\": 0,\n",
    "            \"joint_f1\": 0,\n",
    "            \"joint_prec\": 0,\n",
    "            \"joint_recall\": 0})\n",
    "        \n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        id = batch.pop('id')\n",
    "        answer = batch.pop('answer')\n",
    "        if 'sentence_num' in batch:\n",
    "            sentence_num = batch.pop('sentence_num')\n",
    "            sent_offsets = 0\n",
    "        batch = move_to_cuda(batch)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        start_logits = outputs['start_qa_logits']\n",
    "        end_logits = outputs['end_qa_logits']\n",
    "\n",
    "        if 'sentence_select' in outputs:\n",
    "            sentence_select = torch.argmax(outputs['sentence_select'], axis=-1)\n",
    "        total_num += start_logits.shape[0]\n",
    "        for j in range(start_logits.shape[0]):\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][j].tolist())\n",
    "            pred_answer = None\n",
    "            if 'sentence_select' in outputs:\n",
    "                output_answer_type = outputs['output_answer_type'][j]\n",
    "                sentence = sentence_num[j]\n",
    "                ans_type = torch.argmax(output_answer_type).item()\n",
    "                if ans_type == 0:\n",
    "                    pred_answer = 'no'\n",
    "                elif ans_type == 1:\n",
    "                    pred_answer = 'yes'\n",
    "                \n",
    "                tp, fp, fn = 0, 0, 0\n",
    "                for s in range(sentence):\n",
    "                    if sentence_select[s + sent_offsets] == 1:\n",
    "                        if batch['sentence_labels'][s + sent_offsets] == 1:\n",
    "                            tp += 1\n",
    "                        else:\n",
    "                            fp += 1\n",
    "                    elif batch['sentence_labels'][s + sent_offsets] == 1:\n",
    "                        fn += 1\n",
    "                sent_offsets += sentence\n",
    "                sp_prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "                sp_recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "                sp_f1 = 2 * sp_prec * sp_recall / (sp_prec + sp_recall) if sp_prec + sp_recall > 0 else 0.0\n",
    "                sp_em = 1.0 if fp + fn < 1 else 0.0\n",
    "                metrics[\"sp_em\"] += sp_em\n",
    "                metrics[\"sp_f1\"] += sp_f1\n",
    "                metrics[\"sp_prec\"] += sp_prec\n",
    "                metrics[\"sp_recall\"] += sp_recall\n",
    "                \n",
    "            if pred_answer == None:\n",
    "                answer_tokens = all_tokens[torch.argmax(start_logits[j]) : torch.argmax(end_logits[j]) + 1]\n",
    "                pred_answer = tokenizer.decode(\n",
    "                    tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "                )\n",
    "                pred_answer = normalize_answer(pred_answer)\n",
    "\n",
    "            ground_truth_answer = normalize_answer(answer[j])\n",
    "\n",
    "            pred_list[id[j]] = pred_answer\n",
    "            em = compute_exact(ground_truth_answer, pred_answer)\n",
    "            f1, precision, recall = compute_f1(ground_truth_answer, pred_answer)\n",
    "            metrics[\"em\"] += em\n",
    "            metrics[\"f1\"] += f1\n",
    "            metrics[\"prec\"] += precision\n",
    "            metrics[\"recall\"] += recall\n",
    "\n",
    "            if args.dataset_type == 'hotpot':\n",
    "                joint_prec = precision * sp_prec\n",
    "                joint_recall = recall * sp_recall\n",
    "                if joint_prec + joint_recall > 0:\n",
    "                    joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)\n",
    "                else:\n",
    "                    joint_f1 = 0.0\n",
    "                joint_em = em * sp_em\n",
    "                metrics[\"joint_em\"] += joint_em\n",
    "                metrics[\"joint_f1\"] += joint_f1\n",
    "                metrics[\"joint_prec\"] += joint_prec\n",
    "                metrics[\"joint_recall\"] += joint_recall\n",
    "\n",
    "    for k, v in metrics.items():\n",
    "        metrics[k] = v / total_num\n",
    "    logger.info(f\"evaluated {len(eval_dataloader)} examples...\")\n",
    "    logger.info(f\"performance: {metrics}\")\n",
    "    model.train()\n",
    "    return {'f1':metrics['f1'] if 'joint_f1' not in metrics else metrics['joint_f1'],'pred_list': pred_list}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train beam retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--no_cuda] [--local_rank LOCAL_RANK]\n",
      "                             [--model_name MODEL_NAME] [--beam_size BEAM_SIZE]\n",
      "                             [--use_flash_attention]\n",
      "                             [--flash_attention_type FLASH_ATTENTION_TYPE]\n",
      "                             [--dataset_type DATASET_TYPE]\n",
      "                             [--mean_passage_len MEAN_PASSAGE_LEN]\n",
      "                             [--tokenizer_path TOKENIZER_PATH]\n",
      "                             [--init_checkpoint INIT_CHECKPOINT]\n",
      "                             [--max_seq_len MAX_SEQ_LEN]\n",
      "                             [--use_negative_sampling] [--fp16]\n",
      "                             [--predict_batch_size PREDICT_BATCH_SIZE]\n",
      "                             [--train_file TRAIN_FILE]\n",
      "                             [--predict_file PREDICT_FILE]\n",
      "                             [--num_workers NUM_WORKERS] [--do_train]\n",
      "                             [--do_predict] [--learning_rate LEARNING_RATE]\n",
      "                             [--warmupsteps WARMUPSTEPS]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--accumulate_gradients ACCUMULATE_GRADIENTS]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--gradient_checkpointing] [--prefix PREFIX]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--temperature TEMPERATURE]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--adam_epsilon ADAM_EPSILON] [--seed SEED]\n",
      "                             [--eval_period EVAL_PERIOD]\n",
      "                             [--eval_period_ratio EVAL_PERIOD_RATIO]\n",
      "                             [--log_period_ratio LOG_PERIOD_RATIO]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--stop-drop STOP_DROP] [--use-adam]\n",
      "                             [--warmup-ratio WARMUP_RATIO]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\12609\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-155367fIAZ1Wsd239.json could match --flash_attention_type, --fp16\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from datetime import date\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoConfig, AutoTokenizer, AutoModel,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "from retrieval.retriever_model import Retriever\n",
    "from retrieval.datasets import HotpotQADataset, collate_fn\n",
    "from utils.utils import load_saved, move_to_cuda, AverageMeter\n",
    "from retrieval.config import train_args\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def main():\n",
    "    args = train_args()\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    if args.fp16:\n",
    "        # import apex\n",
    "        # apex.amp.register_half_function(torch, 'einsum')\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        scaler = GradScaler()\n",
    "    date_curr = date.today().strftime(\"%m-%d-%Y\")\n",
    "    model_name = f\"{args.prefix}-seed{args.seed}-bsz{args.train_batch_size}-fp16{args.fp16}-lr{args.learning_rate}-decay{args.weight_decay}-warm{args.warmup_ratio}-valbsz{args.predict_batch_size}\"\n",
    "    args.output_dir = os.path.join(args.output_dir, date_curr, model_name)\n",
    "    tb_path = os.path.join(args.output_dir, \"tblogs\")\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        print(\n",
    "            f\"output directory {args.output_dir} already exists and is not empty.\")\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "    if not os.path.exists(tb_path):\n",
    "        os.makedirs(tb_path, exist_ok=True)\n",
    "\n",
    "    tb_logger = SummaryWriter(tb_path)\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO,\n",
    "                        handlers=[logging.FileHandler(os.path.join(args.output_dir, \"log.txt\")),\n",
    "                                  logging.StreamHandler()])\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(args)\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\",\n",
    "                device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "    if args.accumulate_gradients < 1:\n",
    "        raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
    "            args.accumulate_gradients))\n",
    "\n",
    "    args.train_batch_size = int(\n",
    "        args.train_batch_size / args.accumulate_gradients)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab                          \n",
    "\n",
    "    bert_config = AutoConfig.from_pretrained(args.model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\n",
    "    bert_config.cls_token_id = tokenizer.cls_token_id\n",
    "    bert_config.sep_token_id = tokenizer.sep_token_id\n",
    "    if args.use_flash_attention:\n",
    "        bert_config.use_memorry_efficient_attention = True\n",
    "        bert_config.flash_attention_type = args.flash_attention_type\n",
    "    model = Retriever(bert_config, args.model_name, AutoModel,\n",
    "                      max_seq_len=args.max_seq_len,mean_passage_len=args.mean_passage_len, beam_size=args.beam_size, use_negative_sampling=args.use_negative_sampling,\n",
    "                      gradient_checkpointing=args.gradient_checkpointing, use_label_order=(args.dataset_type=='musique'))\n",
    "\n",
    "    eval_dataset = HotpotQADataset(\n",
    "    tokenizer, args.predict_file, args.max_seq_len, type=args.dataset_type)\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=args.predict_batch_size, pin_memory=True,\n",
    "    num_workers=args.num_workers, collate_fn=collate_fn)\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "\n",
    "    if args.do_train and args.max_seq_len > bert_config.max_position_embeddings:\n",
    "        raise ValueError(\n",
    "            \"Cannot use sequence length %d because the BERT model \"\n",
    "            \"was only trained up to sequence length %d\" %\n",
    "            (args.max_seq_len, bert_config.max_position_embeddings))\n",
    "\n",
    "    if args.local_rank == -1 or args.local_rank == 0:\n",
    "        logger.info(f\"Num of dev batches: {len(eval_dataloader)}\")\n",
    "\n",
    "    if args.init_checkpoint != \"\":\n",
    "        if args.local_rank == -1 or args.local_rank == 0:\n",
    "            logger.info(f\"begin load trained model from :{args.init_checkpoint}\")\n",
    "        model = load_saved(model, args.init_checkpoint)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if args.local_rank == -1 or args.local_rank == 0:\n",
    "        logger.info(f\"number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    if args.do_train:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = Adam(optimizer_parameters,\n",
    "                         lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "\n",
    "    if args.do_train:\n",
    "        global_step = 0  # gradient update step\n",
    "        batch_step = 0  # forward batch count\n",
    "        best_f1 = 0\n",
    "        train_loss_meter = AverageMeter()\n",
    "        model.train()\n",
    "\n",
    "        train_dataset = HotpotQADataset(tokenizer, args.train_file, args.max_seq_len, type=args.dataset_type)\n",
    "        if args.local_rank != -1:\n",
    "            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, pin_memory=True,\n",
    "                                        num_workers=args.num_workers, sampler=train_sampler, collate_fn=collate_fn)\n",
    "        else:\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, pin_memory=True,\n",
    "                                        num_workers=args.num_workers, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "        \n",
    "        t_total = len(train_dataloader) // args.accumulate_gradients * args.num_train_epochs\n",
    "        warmup_steps = t_total * args.warmup_ratio\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        log_steps = int(len(train_dataloader) // args.accumulate_gradients * args.log_period_ratio)\n",
    "        eval_steps = int(len(train_dataloader) // args.accumulate_gradients * args.eval_period_ratio)\n",
    "\n",
    "        if args.local_rank == -1 or args.local_rank == 0:\n",
    "            logger.info(f'Start training.... log_steps:{log_steps}, eval_steps:{eval_steps}')\n",
    "        for epoch in range(int(args.num_train_epochs)):\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                batch_step += 1\n",
    "                id = batch.pop('id')\n",
    "                batch = move_to_cuda(batch)\n",
    "                if args.fp16:\n",
    "                    with autocast():\n",
    "                        loss = model(**batch)['loss']\n",
    "                else:\n",
    "                    loss = model(**batch)['loss']\n",
    "                loss = loss.sum()\n",
    "                if args.accumulate_gradients > 1:\n",
    "                    loss = loss / args.accumulate_gradients\n",
    "                if args.fp16:\n",
    "                    # with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    #     scaled_loss.backward()\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                train_loss_meter.update(loss.item())\n",
    "\n",
    "                if (batch_step + 1) % args.accumulate_gradients == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        model.parameters(), args.max_grad_norm)\n",
    "                    if args.fp16:\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     amp.master_params(optimizer), args.max_grad_norm)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     model.parameters(), args.max_grad_norm)\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    if args.local_rank == -1 or args.local_rank == 0:\n",
    "                        tb_logger.add_scalar('batch_train_loss',\n",
    "                                            loss.item(), global_step)\n",
    "                        tb_logger.add_scalar('smoothed_train_loss',\n",
    "                                            train_loss_meter.avg, global_step)\n",
    "\n",
    "                    if global_step % log_steps == 0 and (args.local_rank == -1 or args.local_rank == 0):\n",
    "                        logger.info(\"Step %d Train loss %.8f on epoch=%d, best_metric=%.3f\" % (\n",
    "                        global_step, train_loss_meter.avg, epoch, best_f1))\n",
    "\n",
    "                    if args.eval_period_ratio > 0 and global_step % eval_steps == 0 and (args.local_rank == -1 or args.local_rank == 0):\n",
    "\n",
    "                        metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "                        pred_list = metric['pred_list']\n",
    "                        metric = metric['em']\n",
    "                        logger.info(\"Step %d Train loss %.8f score %.3f on epoch=%d\" % (\n",
    "                        global_step, train_loss_meter.avg, metric, epoch))\n",
    "\n",
    "                        tb_logger.add_scalar('em',\n",
    "                                            metric, global_step)\n",
    "                        if best_f1 < metric:\n",
    "                            logger.info(\"Saving model with best score %.3f -> score %.3f on epoch=%d\" %\n",
    "                                        (best_f1, metric, epoch))\n",
    "                            torch.save(model.state_dict(), os.path.join(\n",
    "                                args.output_dir, f\"checkpoint_best.pt\"))\n",
    "                            best_f1 = metric\n",
    "                            json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_best.json\"), 'w'))\n",
    "\n",
    "            if args.local_rank == -1 or args.local_rank == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(\n",
    "                    args.output_dir, f\"checkpoint_last.pt\"))\n",
    "                metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "                pred_list = metric['pred_list']\n",
    "                metric = metric['em']\n",
    "                json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_last.json\"), 'w'))\n",
    "                logger.info(\"Step %d Train loss %.8f f1_score %.8f on epoch=%d\" % (\n",
    "                    global_step, train_loss_meter.avg, metric, epoch))\n",
    "\n",
    "                tb_logger.add_scalar('em',\n",
    "                                            metric, global_step)\n",
    "                \n",
    "                if best_f1 < metric:\n",
    "                    logger.info(\"Saving model with best score %.3f -> score %.3f on epoch=%d\" %\n",
    "                                (best_f1, metric, epoch))\n",
    "                    torch.save(model.state_dict(), os.path.join(\n",
    "                        args.output_dir, f\"checkpoint_best.pt\"))\n",
    "                    best_f1 = metric\n",
    "                    json.dump(pred_list, open(os.path.join(args.output_dir, \"pred_best.json\"), 'w'))\n",
    "                \n",
    "\n",
    "        logger.info(\"Training finished!\")\n",
    "\n",
    "    elif args.do_predict:\n",
    "        metric = predict(tokenizer, model, eval_dataloader, logger, args)\n",
    "        logger.info(f\"test performance {metric['em']}\")\n",
    "        json.dump(metric['pred_list'], open(os.path.join(args.output_dir, \"pred.json\"), 'w'))\n",
    "\n",
    "def calculate_em_f1(predicted_support_idxs, gold_support_idxs):\n",
    "    # Taken from hotpot_eval\n",
    "    cur_sp_pred = set(map(int, predicted_support_idxs))\n",
    "    gold_sp_pred = set(map(int, gold_support_idxs))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "\n",
    "    # In case everything is empty, set both f1, em to be 1.0.\n",
    "    # Without this change, em gets 1 and f1 gets 0\n",
    "    if not cur_sp_pred and not gold_sp_pred:\n",
    "        f1, em = 1.0, 1.0\n",
    "        f1, em = 1.0, 1.0\n",
    "    return f1, em\n",
    "\n",
    "def predict(tokenizer, model, eval_dataloader, logger, args):\n",
    "    model.eval()\n",
    "    logger.info(\"begin evaluation\")\n",
    "    em_tot, f1_tot = [], []\n",
    "    pred_list = {}\n",
    "    for i, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        id = batch.pop('id')\n",
    "        batch = move_to_cuda(batch)\n",
    "        with torch.no_grad():\n",
    "            current_preds = model(**batch)['current_preds']\n",
    "        pred_list[id[0]] = current_preds[0]\n",
    "        f1, em = calculate_em_f1(current_preds[0], batch['sf_idx'][0])\n",
    "        em_tot.append(em)\n",
    "        f1_tot.append(f1)\n",
    "        \n",
    "    em = sum(em_tot) / len(em_tot)\n",
    "    f1 = sum(f1_tot) / len(f1_tot)\n",
    "    logger.info(f\"evaluated {len(eval_dataloader)} examples...\")\n",
    "    logger.info(f\"performance: em: {em}, f1: {f1}\")\n",
    "    model.train()\n",
    "    return {'em':em, 'f1': f1, 'pred_list': pred_list}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_moel_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'project/hotpotqa/retr_beamsize2_793.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mload_saved\u001b[1;34m(model, path, exact)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'project/hotpotqa/retr_beamsize2_793.pt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 437\u001b[0m\n\u001b[0;32m    435\u001b[0m musique_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(test_file_path)\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m    436\u001b[0m test_raw_data \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m musique_data]\n\u001b[1;32m--> 437\u001b[0m retr_json \u001b[38;5;241m=\u001b[39m get_retr_output(test_raw_data, is_dev\u001b[38;5;241m=\u001b[39mis_dev, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m, beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    438\u001b[0m get_reader_qa_output(retr_json, test_raw_data, is_dev\u001b[38;5;241m=\u001b[39mis_dev, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m, answer_merge\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 167\u001b[0m, in \u001b[0;36mget_retr_output\u001b[1;34m(test_raw_data, type, is_dev, beam_size)\u001b[0m\n\u001b[0;32m    165\u001b[0m config\u001b[38;5;241m.\u001b[39msep_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39msep_token_id\n\u001b[0;32m    166\u001b[0m re_model \u001b[38;5;241m=\u001b[39m Retriever(config, re_model_path, encoder_class\u001b[38;5;241m=\u001b[39mAutoModel, mean_passage_len\u001b[38;5;241m=\u001b[39mmean_passage_len, beam_size\u001b[38;5;241m=\u001b[39mbeam_size, gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 167\u001b[0m re_model \u001b[38;5;241m=\u001b[39m load_saved(re_model, re_checkpoint)\n\u001b[0;32m    168\u001b[0m re_model \u001b[38;5;241m=\u001b[39m re_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    169\u001b[0m re_model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m, in \u001b[0;36mload_saved\u001b[1;34m(model, path, exact)\u001b[0m\n\u001b[0;32m     21\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(x):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x[\u001b[38;5;241m7\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'project/hotpotqa/retr_beamsize2_793.pt'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "from qa.reader_model import Reader\n",
    "from retrieval.retriever_model import Retriever\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "def load_saved(model, path, exact=True):\n",
    "    try:\n",
    "        state_dict = torch.load(path)\n",
    "    except:\n",
    "        state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "    def filter(x):\n",
    "        return x[7:] if x.startswith('module.') else x\n",
    "\n",
    "    if exact:\n",
    "        state_dict = {filter(k): v for (k, v) in state_dict.items()}\n",
    "    else:\n",
    "        state_dict = {filter(k): v for (k, v) in state_dict.items() if filter(k) in model.state_dict()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def move_to_cuda(sample):\n",
    "    if len(sample) == 0:\n",
    "        return {}\n",
    "\n",
    "    def _move_to_cuda(maybe_tensor):\n",
    "        if torch.is_tensor(maybe_tensor):\n",
    "            return maybe_tensor.cuda()\n",
    "        elif isinstance(maybe_tensor, dict):\n",
    "            return {\n",
    "                key: _move_to_cuda(value)\n",
    "                for key, value in maybe_tensor.items()\n",
    "            }\n",
    "        elif isinstance(maybe_tensor, list):\n",
    "            return [_move_to_cuda(x) for x in maybe_tensor]\n",
    "        else:\n",
    "            return maybe_tensor\n",
    "\n",
    "    return _move_to_cuda(sample)\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def calculate_em_f1(predicted_support_idxs, gold_support_idxs):\n",
    "    # Taken from hotpot_eval\n",
    "    cur_sp_pred = set(map(int, predicted_support_idxs))\n",
    "    gold_sp_pred = set(map(int, gold_support_idxs))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "\n",
    "    # In case everything is empty, set both f1, em to be 1.0.\n",
    "    # Without this change, em gets 1 and f1 gets 0\n",
    "    if not cur_sp_pred and not gold_sp_pred:\n",
    "        f1, em = 1.0, 1.0\n",
    "        f1, em = 1.0, 1.0\n",
    "    return f1, em\n",
    "\n",
    "def normalize_sp(sps):\n",
    "    new_sps = []\n",
    "    for sp in sps:\n",
    "        sp = list(sp)\n",
    "        sp[0] = sp[0].lower()\n",
    "        new_sps.append(sp)\n",
    "    return new_sps\n",
    "\n",
    "\n",
    "def update_sp(prediction, gold):\n",
    "    cur_sp_pred = normalize_sp(set(map(tuple, prediction)))\n",
    "    gold_sp_pred = normalize_sp(set(map(tuple, gold)))\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for e in cur_sp_pred:\n",
    "        if e in gold_sp_pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for e in gold_sp_pred:\n",
    "        if e not in cur_sp_pred:\n",
    "            fn += 1\n",
    "    prec = 1.0 * tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    recall = 1.0 * tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1 = 2 * prec * recall / (prec + recall) if prec + recall > 0 else 0.0\n",
    "    em = 1.0 if fp + fn == 0 else 0.0\n",
    "    return em, f1\n",
    "\n",
    "def get_retr_output(test_raw_data, type='musique', is_dev=True, beam_size=1):\n",
    "    retr_dic = {}\n",
    "    if type == '2wiki':\n",
    "        re_tokenizer_path = 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0'\n",
    "    else:\n",
    "        re_tokenizer_path = 'MoritzLaurer/deberta-v3-large-zeroshot-v2.0'\n",
    "    re_model_path = re_tokenizer_path\n",
    "    if type == 'musique':\n",
    "        re_checkpoint = 'project/hotpotqa/retr_beamsize2_793.pt'\n",
    "    else:\n",
    "        re_checkpoint = 'project/hotpotqa/2407_codes/output/07-24-2023/train_2wiki_continue_training-seed42-bsz8-fp16True-lr1e-05-decay0.0-warm0.1-valbsz1/checkpoint_best.pt'\n",
    "    pred_filename = f\"pred_test_{type}_v0_retr.json\"\n",
    "    max_len = 512\n",
    "    mean_passage_len = 250 if type=='hotpot' else 120 \n",
    "    device = torch.device(\"cuda\", 1)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(re_tokenizer_path)\n",
    "    config = AutoConfig.from_pretrained(re_tokenizer_path)\n",
    "    config.cls_token_id = tokenizer.cls_token_id\n",
    "    config.sep_token_id = tokenizer.sep_token_id\n",
    "    re_model = Retriever(config, re_model_path, encoder_class=AutoModel, mean_passage_len=mean_passage_len, beam_size=beam_size, gradient_checkpointing=True)\n",
    "    re_model = load_saved(re_model, re_checkpoint)\n",
    "    re_model = re_model.to(device)\n",
    "    re_model.eval()\n",
    "    if is_dev:\n",
    "        em_tot, f1_tot = [], []\n",
    "    # get tensors\n",
    "    for sample in tqdm(test_raw_data, desc=\"RE Predicting:\"):\n",
    "        question = sample['question']\n",
    "        if question.endswith(\"?\"):\n",
    "            question = question[:-1]\n",
    "        id = sample['id'] if type == 'musique' else sample['_id']\n",
    "        q_codes = tokenizer.encode(question, add_special_tokens=False, return_tensors=\"pt\", truncation=True, max_length=max_len).squeeze(0)\n",
    "        c_codes = []\n",
    "        if is_dev:\n",
    "            sf_idx = []\n",
    "            sp_title_set = set()\n",
    "        if type == 'hotpot' or type == '2wiki':\n",
    "            for idx, (title, sentences) in enumerate(sample['context']):\n",
    "                if is_dev:\n",
    "                    for sup in sample['supporting_facts']:\n",
    "                        sp_title_set.add(sup[0])\n",
    "                    if title in sp_title_set:\n",
    "                        sf_idx.append(idx)\n",
    "                l = title + \"\".join(sentences)\n",
    "                encoding = tokenizer.encode(l, add_special_tokens=False, return_tensors=\"pt\", truncation=True, max_length=max_len-q_codes.shape[-1]).squeeze(0)\n",
    "                encoding = encoding.to(device)\n",
    "                c_codes.append(encoding)\n",
    "        elif type == 'musique':\n",
    "            # musique\n",
    "            for i, para in enumerate(sample['paragraphs']):\n",
    "                if is_dev:\n",
    "                    if para['is_supporting']:\n",
    "                        sf_idx.append(i)\n",
    "                l = para['title'] + '.' + para['paragraph_text']\n",
    "                encoding = tokenizer.encode(l, add_special_tokens=False, return_tensors=\"pt\", truncation=True, max_length=max_len-q_codes.shape[-1]).squeeze(0)\n",
    "                encoding = encoding.to(device)\n",
    "                c_codes.append(encoding)\n",
    "        q_codes = q_codes.to(device)\n",
    "        q_codes_input = [q_codes]\n",
    "        c_codes_input = [c_codes]\n",
    "        hop = int(id[0]) if type == 'musique' else 2\n",
    "        if type == '2wiki' and sample['type'] == 'bridge_comparison':\n",
    "            hop = 4\n",
    "        with torch.no_grad():\n",
    "            current_preds = re_model(q_codes_input, c_codes_input, [] if not is_dev else sf_idx, hop=hop)['current_preds']\n",
    "        retr_dic[id] = current_preds[0]\n",
    "        if is_dev:\n",
    "            f1, em = calculate_em_f1(current_preds[0], sf_idx)\n",
    "            em_tot.append(em)\n",
    "            f1_tot.append(f1)\n",
    "    if is_dev:\n",
    "        print(f\"em:{sum(em_tot) / len(em_tot)}, f1:{sum(f1_tot) / len(f1_tot)}\")\n",
    "    with open(pred_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(retr_dic, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"retr evaluation finished!\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return retr_dic\n",
    "\n",
    "def merge_find_ans(start_logits, end_logits, ids, punc_token_list, topk=5, max_ans_len=20):\n",
    "    def is_too_long(span_id, punc_token_list):\n",
    "        for punc_token_id in punc_token_list:\n",
    "            if punc_token_id in span_id:\n",
    "                return True\n",
    "        return False\n",
    "    start_candidate_val, start_candidate_idx = start_logits.topk(topk, dim=-1)\n",
    "    end_candidate_val, end_candidate_idx = end_logits.topk(topk, dim=-1)\n",
    "    pointer_s, pointer_e = 0, 0\n",
    "    start = start_candidate_idx[pointer_s].item()\n",
    "    end = end_candidate_idx[pointer_e].item()\n",
    "    span_id = ids[start: end + 1]\n",
    "    while start > end or (end - start) > max_ans_len or is_too_long(span_id, punc_token_list):\n",
    "        if start_candidate_val[pointer_s] > end_candidate_val[pointer_e]:\n",
    "            pointer_e += 1\n",
    "        else:\n",
    "            pointer_s += 1\n",
    "        if pointer_s >= topk or pointer_e >= topk:\n",
    "            break\n",
    "        start = start_candidate_idx[pointer_s].item()\n",
    "        end = end_candidate_idx[pointer_e].item()\n",
    "        span_id = ids[start: end + 1]\n",
    "    return span_id\n",
    "\n",
    "def get_reader_qa_output(retr_pred_dic, test_raw_data, type='musique', is_dev=True, answer_merge=False, topk=5):\n",
    "    qa_tokenizer_path = \"deepset/deberta-v3-large-squad2\"\n",
    "    qa_model_path = qa_tokenizer_path\n",
    "    if type == '2wiki':\n",
    "        qa_checkpoint = 'project/hotpotqa/2107_codes/output/07-21-2023/2wiki_multi_reader_large-seed42-bsz4-fp16True-lr1e-05-decay0.0-warm0.1-valbsz32/checkpoint_best.pt'\n",
    "    else:\n",
    "        qa_checkpoint = \"project/hotpotqa/3107_codes/output/08-01-2023/musique_reader_deberta_large_from_scratch-seed42-bsz8-fp16True-lr6e-06-decay0.0-warm0.1-valbsz32/checkpoint_best.pt\"\n",
    "    pred_filename = f\"sorted_pred_{'dev' if is_dev else 'test'}_{type}_v0_retrlarge_793_qalarge_70_{'merged' if answer_merge else 'no_merged'}.{'jsonl' if type=='musique' else 'json'}\"\n",
    "    max_len = 1024\n",
    "    device = torch.device(\"cuda\", 1)\n",
    "    config = AutoConfig.from_pretrained(qa_model_path)\n",
    "    config.max_position_embeddings = max_len\n",
    "    tokenizer = AutoTokenizer.from_pretrained(qa_tokenizer_path)\n",
    "    type = 'hotpot' if type == '2wiki' else type\n",
    "    if type == 'hotpot':\n",
    "        SEP = \"</e>\"\n",
    "        DOC = \"</d>\"\n",
    "        tokenizer.add_tokens([SEP, DOC])\n",
    "        SEP_id = tokenizer.convert_tokens_to_ids(SEP)\n",
    "        DOC_id = tokenizer.convert_tokens_to_ids(DOC)\n",
    "        sp_pred = {}\n",
    "        ans_pred = {}\n",
    "    qa_model = Reader(config, qa_model_path, len(tokenizer) if ('deberta' not in qa_tokenizer_path) else 0)\n",
    "    qa_model = load_saved(qa_model, qa_checkpoint)\n",
    "    qa_model = qa_model.to(device)\n",
    "    qa_model.eval()\n",
    "    pred_list = []\n",
    "    if is_dev:\n",
    "        em_tot, f1_tot = [], []\n",
    "        if type == 'hotpot':\n",
    "            sp_em_tot, sp_f1_tot = [], []\n",
    "    # get tensors\n",
    "    for sample in tqdm(test_raw_data, desc=\"QA Predicting:\"):\n",
    "        question = sample['question']\n",
    "        if question.endswith(\"?\"):\n",
    "            question = question[:-1]\n",
    "        id = sample['id'] if type == 'musique' else sample['_id']\n",
    "        q_codes = tokenizer.encode(question, add_special_tokens=False, truncation=True, max_length=max_len)\n",
    "        sp_list = retr_pred_dic[id]\n",
    "        idx2title = {}\n",
    "        c_codes = []\n",
    "        if type == 'hotpot':\n",
    "            # hotpot format\n",
    "            sts2title = {}\n",
    "            sts2idx = {}\n",
    "            sts_idx = 0\n",
    "            sentence_label = []\n",
    "            if is_dev:\n",
    "                sp_title_set = {}\n",
    "                for sup in sample['supporting_facts']:\n",
    "                    if sup[0] not in sp_title_set:\n",
    "                        sp_title_set[sup[0]] = []\n",
    "                    sp_title_set[sup[0]].append(sup[1])\n",
    "            for idx, (title, sentences) in enumerate(sample['context']):\n",
    "                if idx in sp_list:\n",
    "                    idx2title[idx] = title\n",
    "                    l = DOC + \" \" + title\n",
    "                    for idx2, c in enumerate(sentences):\n",
    "                        l += (SEP + \" \" + c)\n",
    "                        # sts2title[sts_idx] = title\n",
    "                        # sts2idx[sts_idx] = idx2\n",
    "                        # if is_dev:\n",
    "                        #     if title in sp_title_set and idx2 in sp_title_set[title]:\n",
    "                        #         sentence_label.append(1)\n",
    "                        #     else:\n",
    "                        #         sentence_label.append(0)\n",
    "                        # sts_idx += 1\n",
    "                    encoding = tokenizer.encode(l, add_special_tokens=False, truncation=True, max_length=max_len-len(q_codes))\n",
    "                    c_codes.append(encoding)\n",
    "        elif type == 'musique':\n",
    "            # musique\n",
    "            for i, para in enumerate(sample['paragraphs']):\n",
    "                if i in sp_list:\n",
    "                    l = para['title'] + '.' + para['paragraph_text']\n",
    "                    encoding = tokenizer.encode(l, add_special_tokens=False, truncation=True, max_length=max_len-len(q_codes))\n",
    "                    c_codes.append(encoding)\n",
    "        total_len = len(q_codes) + sum([len(item) for item in c_codes])\n",
    "        context_ids = [tokenizer.cls_token_id] + q_codes\n",
    "        avg_len = (max_len - 2 - len(q_codes)) // len(c_codes)\n",
    "        \n",
    "        if type == 'hotpot':\n",
    "            sp_list.sort() # only hotpot format, for sp prediction, align sentence order and passages order\n",
    "        for idx, item in enumerate(c_codes):\n",
    "            if total_len > max_len - 2:\n",
    "                # 可能把答案截断\n",
    "                item = item[:avg_len]\n",
    "            if type == 'hotpot':\n",
    "                sts_idx_local = 0\n",
    "                for i in range(len(item)):\n",
    "                    if item[i] == SEP_id:\n",
    "                        sts2title[sts_idx] = idx2title[sp_list[idx]]\n",
    "                        sts2idx[sts_idx] = sts_idx_local\n",
    "                        sts_idx += 1\n",
    "                        sts_idx_local += 1\n",
    "            context_ids.extend(item)\n",
    "        context_ids = context_ids[:max_len - 1] + [tokenizer.sep_token_id]\n",
    "        pred_answer = None\n",
    "        input_ids = torch.tensor(context_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        attention_mask = torch.ones([1, len(context_ids)], dtype=torch.long, device=device)\n",
    "        if type == 'hotpot':\n",
    "            SEP_index = []\n",
    "            for i in range(len(context_ids)):\n",
    "                if context_ids[i] == SEP_id:\n",
    "                    SEP_index.append(i)\n",
    "            SEP_index = torch.LongTensor([SEP_index]).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = qa_model(input_ids, attention_mask, sentence_index=SEP_index[0])\n",
    "            sentence_select = torch.argmax(outputs['sentence_select'], dim=-1)\n",
    "            assert sentence_select.shape[-1] == len(sts2idx)\n",
    "            output_answer_type = outputs['output_answer_type']\n",
    "            ans_type = torch.argmax(output_answer_type).item()\n",
    "            if ans_type == 0:\n",
    "                pred_answer = 'no'\n",
    "            elif ans_type == 1:\n",
    "                pred_answer = 'yes'\n",
    "            sp = []\n",
    "            sts_idx = 0\n",
    "            for s in range(len(sentence_select)):\n",
    "                if sentence_select[s] == 1:\n",
    "                    sp.append([sts2title[s], sts2idx[s]])\n",
    "            sp_pred[id] = sp\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = qa_model(input_ids, attention_mask)\n",
    "        start_logits = outputs['start_qa_logits'][0]\n",
    "        end_logits = outputs['end_qa_logits'][0]\n",
    "        input_ids = input_ids[0]\n",
    "\n",
    "        if pred_answer is None:\n",
    "            if answer_merge:\n",
    "                punc_token_list = tokenizer.convert_tokens_to_ids(['[CLS]', '?'])\n",
    "                if type == 'hotpot':\n",
    "                    punc_token_list.extend([SEP_id, DOC_id])\n",
    "                span_id = merge_find_ans(start_logits, end_logits, input_ids.tolist(),punc_token_list, topk=topk)\n",
    "                pred_answer = tokenizer.decode(span_id)\n",
    "            else:\n",
    "                all_tokens = tokenizer.convert_ids_to_tokens(input_ids.tolist())\n",
    "\n",
    "                answer_tokens = all_tokens[torch.argmax(start_logits) : torch.argmax(end_logits) + 1]\n",
    "                pred_answer = tokenizer.decode(\n",
    "                    tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "                )\n",
    "\n",
    "        pred_answer = normalize_answer(pred_answer)\n",
    "        if type == 'hotpot':\n",
    "            ans_pred[id] = pred_answer\n",
    "\n",
    "        else:\n",
    "            pred_list.append({'id':id, 'predicted_answer': pred_answer, 'predicted_support_idxs': sp_list, 'predicted_answerable':True})\n",
    "        \n",
    "        if is_dev:\n",
    "            ground_truth_answer = sample['answer']\n",
    "            ground_truth_answer = normalize_answer(ground_truth_answer)\n",
    "\n",
    "            em = compute_exact(ground_truth_answer, pred_answer)\n",
    "            f1 = compute_f1(ground_truth_answer, pred_answer)\n",
    "            em_tot.append(em)\n",
    "            f1_tot.append(f1)\n",
    "\n",
    "            if type == 'hotpot':\n",
    "                sp_em, sp_f1 = update_sp(sp, sample['supporting_facts'])\n",
    "                sp_em_tot.append(sp_em)\n",
    "                sp_f1_tot.append(sp_f1)\n",
    "                print(f\"sp em:{sum(sp_em_tot) / len(sp_em_tot)}, sp f1:{sum(sp_f1_tot) / len(sp_f1_tot)}\")\n",
    "    if is_dev:\n",
    "        print(f\"em:{sum(em_tot) / len(em_tot)}, f1:{sum(f1_tot) / len(f1_tot)}\")\n",
    "        if type == 'hotpot':\n",
    "            print(f\"sp em:{sum(sp_em_tot) / len(sp_em_tot)}, sp f1:{sum(sp_f1_tot) / len(sp_f1_tot)}\")\n",
    "    if type == 'musique':\n",
    "        with jsonlines.open(pred_filename, \"w\") as wfd:\n",
    "            for data in pred_list:\n",
    "                wfd.write(data)\n",
    "    else:\n",
    "        with open(pred_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"answer\": ans_pred, \"sp\": sp_pred}, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"evaluation finished!\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    is_dev = True\n",
    "    type = 'musique'\n",
    "    # with open('project/hotpotqa/source_code/output/07-05-2023/train_2wiki_0-seed42-bsz8-fp16True-lr1e-05-decay0.0-warm0.1-valbsz1/pred_best.json', 'r') as f:\n",
    "    #     retr_json = json.load(f)\n",
    "    test_file_path = f\"F:/public datas/NLP/multi_QA/musique_data_v1.0/data/musique_ans_v1.0_{'dev' if is_dev else 'test'}.jsonl\"\n",
    "    # test_file_path = f\"data/datasets/mrc/2wikimultihop/data/{'dev' if is_dev else 'test'}.json\"\n",
    "    # test_raw_data = json.load(open(test_file_path))\n",
    "    musique_data = open(test_file_path).readlines()\n",
    "    test_raw_data = [json.loads(item) for item in musique_data]\n",
    "    retr_json = get_retr_output(test_raw_data, is_dev=is_dev, type=type, beam_size=2)\n",
    "    get_reader_qa_output(retr_json, test_raw_data, is_dev=is_dev, type=type, answer_merge=True, topk=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
